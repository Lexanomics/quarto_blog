@article{Cui2024,
   abstract = {Generative pretrained models have achieved remarkable success in various domains such as language and computer vision. Specifically, the combination of large-scale diverse datasets and pretrained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between language and cellular biology (in which texts comprise words; similarly, cells are defined by genes), our study probes the applicability of foundation models to advance cellular biology and genetic research. Using burgeoning single-cell sequencing data, we have constructed a foundation model for single-cell biology, scGPT, based on a generative pretrained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT effectively distills critical biological insights concerning genes and cells. Through further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell type annotation, multi-batch integration, multi-omic integration, perturbation response prediction and gene network inference. Pretrained using over 33 million single-cell RNA-sequencing profiles, scGPT is a foundation model facilitating a broad spectrum of downstream single-cell analysis tasks by transfer learning.},
   author = {Haotian Cui and Chloe Wang and Hassaan Maan and Kuan Pang and Fengning Luo and Nan Duan and Bo Wang},
   doi = {10.1038/s41592-024-02201-0},
   issn = {1548-7105},
   journal = {Nature Methods 2024},
   keywords = {Computational models,Machine learning,Software,Transcriptomics},
   month = {2},
   pages = {1-11},
   publisher = {Nature Publishing Group},
   title = {scGPT: toward building a foundation model for single-cell multi-omics using generative AI},
   url = {https://www.nature.com/articles/s41592-024-02201-0},
   year = {2024},
}
